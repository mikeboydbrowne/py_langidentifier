#!/usr/bin/env python

#####################################################################
# default
#
# A very simple solution to the multilingual document language prediction
# task. 
#
# Method: Use monolingual training documents to create a byte n-gram (length specified
# by -n option) dictionary for each language. Then, for each document in the test set,
# maintain a k-dimensional vector where k is the number of possible languages in 
# the language set. For each byte ngram in the test doc, add 1 to the dimension(s) of
# the vector corresponding to a dictionary match for that language. Once we've 
# tallied all the ngrams in the doc, normalize the vector by dividing by its total
# sum across languages.
# 

import optparse
import os
from collections import Counter
import sys


def count_byte_ngrams(infile, n):
    """ Count byte n-grams in infile.
    :param infile: str
    :param n: int
    :return: Counter
    """
    bytecounter = Counter()
    chars = [c for c in open(infile, 'rB').read()]
    for i in range(0, len(chars) + 1 - n):
        bytecounter[tuple(chars[i:i+n])] += 1
    return bytecounter


if __name__ == "__main__":
    optparser = optparse.OptionParser()
    optparser.add_option("-l", "--langfile", dest="langfile", default="data-train/selected_langs", help="File containing all possible languages in set")
    optparser.add_option("-d", "--monodir", dest="monodir", default="data-train/docsMR", help="Directory containing monolingual training data")
    optparser.add_option("-m", "--monometa", dest="monometa", default="data-train/docsMR-meta", help="File containing monolingual metadata")
    optparser.add_option("-t", "--testdir", dest="testdir", default="data-dev", help="Directory containing docs on which to run prediction")
    optparser.add_option("-n", "--ngramlen", dest="ngramlen", type='int', default=3, help="Length of byte n-grams for language dictionaries")
    optparser.add_option("-k", "--vocablen", dest="vocablen", type='int', default=500, help="Number of byte n-grams for each language to retain in dictionary")
    opts = optparser.parse_args()[0]

    # Map languages to docs
    lang_doc = {l.strip(): [] for l in open(opts.langfile, 'rU')}
    for line in open(opts.monometa, 'rU'):
        (docname, _, _, lang, _) = line.split(',')
        lang_doc[lang].append(docname)

    # Create byte n-gram dictionary for each language
    sys.stderr.write("Creating byte n-gram dictionary for languages:\n")
    lang_dicts = {l: Counter() for l in lang_doc.keys()}
    for lang in lang_dicts.keys():
        sys.stderr.write("%s\n" % lang)
        for doc in lang_doc[lang]:
            lang_dicts[lang] += count_byte_ngrams(os.path.join(opts.monodir, doc), opts.ngramlen)
    lang_dicts[lang] = Counter({l: c for l, c in lang_dicts[lang].most_common(opts.vocablen)})

    # Predict languages for each doc in the test set
    language_order = sorted(lang_doc.keys())  # so we can make sure we output in correct order
    sys.stdout.write('docnum ||| %s\n' % ','.join(language_order))

    for i, doc in enumerate(os.listdir(opts.testdir)):  # for each doc in the test set...
        if not doc.startswith('.') and os.path.isfile(os.path.join(opts.testdir, doc)):  # ...ignoring hidden files and folders
            if i % 50 == 0:
                sys.stderr.write("Docs read: %d\n" %i)
            language_counter = Counter({l: 0 for l in lang_dicts})  # setup language vector
            ngrams = count_byte_ngrams(os.path.join(opts.testdir, doc), opts.ngramlen)  # count n-grams
            for lang in language_counter:  # sort through language dictionaries to see if how many ngram matches there are
                language_counter[lang] += sum([1 for grm in ngrams if grm in lang_dicts[lang]])
            total_matches = float(sum(language_counter.values()))  # divide by this so that vector sums to 1
            sys.stdout.write('%s ||| %s\n' % (doc, ','.join([str(matches/total_matches) for matches in [language_counter[lang] for lang in language_order]])))




